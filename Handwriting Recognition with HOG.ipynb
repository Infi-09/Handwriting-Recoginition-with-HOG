{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aeaf32c5",
   "metadata": {},
   "source": [
    "## Handwriting Recognition with HOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcdf1ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the needed libraries\n",
    "import cv2\n",
    "import joblib\n",
    "import mahotas\n",
    "import numpy as np\n",
    "from skimage import feature\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e21c66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the HOG class to extract the features from the Handwritten images\n",
    "class HOG:\n",
    "    def __init__(self, orientations=9, pixelsPerCell=(8, 8), cellsPerBlock=(3, 3), transform=False):\n",
    "        self.orientations = orientations\n",
    "        self.pixelsPerCell = pixelsPerCell\n",
    "        self.cellsPerBlock = cellsPerBlock\n",
    "        self.transform = transform\n",
    "    \n",
    "    def describe(self, image):    \n",
    "        hist = feature.hog(image,\n",
    "                           orientations = self.orientations,\n",
    "                           pixels_per_cell = self.pixelsPerCell,\n",
    "                           cells_per_block = self.cellsPerBlock,\n",
    "                           transform_sqrt = self.transform)\n",
    "        return hist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb61ea6",
   "metadata": {},
   "source": [
    "Sets up __init__ constructor requires four parameters. The first, orientations, defines how many **gradient orientations** will be in each histogram (i.e., the number of bins). The **pixelsPerCell** parameter defines the number of pixels that will fall into each cell. When computing the HOG descriptor over an image, the image will be partitioned into multiple cells, each of size pixelsPerCell × pixelsPerCell.\n",
    "\n",
    "A histogram of gradient magnitudes will then be computed for each cell. HOG will then normalize each of the histograms according to the number of cells that fall into each block using the **cellsPerBlock** argument.\n",
    "\n",
    "Optionally, HOG can apply **power law compression** (taking the log/square-root of the input image), which can lead to better accuracy of the descriptor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9e3c18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_digits(datasetPath):\n",
    "    # Convert text data into dataframe\n",
    "    data = np.genfromtxt(datasetPath, delimiter=\",\", dtype=\"uint8\")\n",
    "    target = data[:, 0]\n",
    "    data = data[:, 1:].reshape(data.shape[0], 28, 28)\n",
    "    return (data, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cb1ac5",
   "metadata": {},
   "source": [
    "Next, the dataset of digits that he can use to extract features from and train his machine learning model. Let's use a sample of the **MNIST digit recognition dataset**.\n",
    "\n",
    "The sample of the dataset consists of **5000 data points**, each with a feature vector of length 784, corresponding to the **28 × 28 grayscale pixel** intensities of the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbd23e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function used to resize the image according to width or height\n",
    "def resize_image(image, width=None, height=None, inter=cv2.INTER_AREA):\n",
    "    (h, w) = image.shape[:2]\n",
    "    dim = None\n",
    "    \n",
    "    if width is None and height is None:\n",
    "        return image\n",
    "    \n",
    "    if width is None:\n",
    "        r = height / float(h)\n",
    "        dim = (int(r * w), height)\n",
    "    else:\n",
    "        r = width / float(w)\n",
    "        dim = (width, int(r * h))\n",
    "        \n",
    "    resized = cv2.resize(image, dim, interpolation=inter)\n",
    "    return resized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60579a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deskew(image, width):\n",
    "    (h, w) = image.shape[:2]\n",
    "    # find moments\n",
    "    moments = cv2.moments(image)\n",
    "    \n",
    "    skew = moments[\"mu11\"] / moments[\"mu02\"]\n",
    "    M = np.float32([[1, skew, -0.5 * w * skew], [0, 1, 0]])\n",
    "    image = cv2.warpAffine(image, M, (w, h), flags=cv2.WARP_INVERSE_MAP|cv2.INTER_LINEAR)\n",
    "    image = resize_image(image, width=width)\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2715cd",
   "metadata": {},
   "source": [
    "This **deskew** function takes two arguments. The first is the image of the digit that is going to be deskewed. The second is the width that the image is going to be resized to.\n",
    "\n",
    "This function grabs the height and width of the image, then the moments of the image are computed. These **moments** contain statistical information regarding the distribution of the location of the white pixels in the image. The skew is computed based on the moments and the warping matrix. This matrix M will be used to deskew the image.\n",
    "\n",
    "The actual deskewing of the image take places on when we call the cv2.warpAffine function. The first argument is the image that is going to be skewed, the second is the matrix M that defines the “direction” in which the image is going to be deskewed, and the third parameter is the resulting width and height of the deskewed image.\n",
    "\n",
    "Finally, the flags parameter controls how the image is going to be deskewed(linear interpolation). The deskewed image is then resized and returned to the caller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b700635",
   "metadata": {},
   "outputs": [],
   "source": [
    "def center_extent(image, size):\n",
    "    (eW, eH) = size\n",
    "    \n",
    "    if image.shape[1] > image.shape[0]:\n",
    "        image = resize_image(image, width = eW)\n",
    "    else:\n",
    "        image = resize_image(image, height = eH)\n",
    "        \n",
    "    extent = np.zeros((eH, eW), dtype = \"uint8\")\n",
    "\n",
    "    offsetX = (eW - image.shape[1]) // 2\n",
    "    offsetY = (eH - image.shape[0]) // 2\n",
    "    extent[offsetY:offsetY+image.shape[0], offsetX:offsetX+image.shape[1]] = image\n",
    "\n",
    "    CM = mahotas.center_of_mass(extent)\n",
    "    (cY, cX) = np. round(CM).astype(\"int32\")\n",
    "    \n",
    "    (dX, dY) = ((size[0] // 2) - cX, (size[1] // 2) - cY)\n",
    "    M = np. float32([[1, 0, dX], [0, 1 , dY]])\n",
    "    \n",
    "    extent = cv2.warpAffine(extent, M, size)\n",
    "    \n",
    "    return extent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853530b8",
   "metadata": {},
   "source": [
    "In this function, First checks to see if the width is greater than the height of the image. If this is the case, the image is resized based on its width.Otherwise the height is greater than the width, so the image must be resized based on its height.\n",
    "\n",
    "Hank notes that these are import ant checks to make. If this checks were not made and the image was always resized on its width than there is a chance that the height could be larger than the width, and thus would not fit into the “extent” of the image.\n",
    "\n",
    "These offsets indicate the starting (x, y) coordinates (y, x order) of where the image will be placed in the extent. The actual extent is using some NumPy array slicing.The next step is to translate the digit so it is placed at the center of the image. For that computes the weighted mean of the white pixels in the image using the center_of_mass function of the mahotas package. This function returns the weighted (x, y) coordinates of the center of the image, then converts these (x, y) coordinates to integers rather than floats, then translates the digit so that it is placed at the center of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00e79c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data using our helper function called load_digits\n",
    "(digits, target) = load_digits(\"data/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba1e4337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the object hog for the HOG class \n",
    "hog = HOG(orientations=18,\n",
    "          pixelsPerCell=(10, 10),\n",
    "          cellsPerBlock=(1, 1),\n",
    "          transform=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "062c92d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "# Loop the image in the dataset \n",
    "for image in digits:\n",
    "    # deskew the image\n",
    "    image = deskew(image, 20)\n",
    "    # place the image at the center\n",
    "    image = center_extent(image, (20, 20))\n",
    "    \n",
    "    # Extract the feature of the image\n",
    "    hist = hog.describe(image)\n",
    "    data.append(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5aedbd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(random_state=42)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calling the LinearSVC\n",
    "model = LinearSVC(random_state=42)\n",
    "# Fit the data\n",
    "model.fit(data, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a091c9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/svm2.cpickle']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model \n",
    "joblib.dump(model, \"models/svm3.cpickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b91cd644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model = joblib.load(\"models/svm.cpickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db8fa674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the image\n",
    "image = cv2.imread(\"images/num.jpeg\")\n",
    "# Convert RGB to gray scale\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "# Blur the image using Gaussian Blur\n",
    "blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "# Edge Detection using Canny \n",
    "edged = cv2.Canny(blurred, 30, 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "998711ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the contuors of the numbers in the image\n",
    "(cnts, _) = cv2.findContours(edged.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "# Sort the contours\n",
    "cnts = sorted([(c, cv2.boundingRect(c)[0]) for c in cnts], key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a188ca3a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 450 features per sample; expecting 72",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-43b2092e06b7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mhist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhog\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthresh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;31m# Predict the digit using the trained model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[0mdigit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mhist\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"I think that number is: {}\"\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdigit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    307\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0mper\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    308\u001b[0m         \"\"\"\n\u001b[1;32m--> 309\u001b[1;33m         \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    310\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    311\u001b[0m             \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    286\u001b[0m         \u001b[0mn_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    287\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mn_features\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 288\u001b[1;33m             raise ValueError(\"X has %d features per sample; expecting %d\"\n\u001b[0m\u001b[0;32m    289\u001b[0m                              % (X.shape[1], n_features))\n\u001b[0;32m    290\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: X has 450 features per sample; expecting 72"
     ]
    }
   ],
   "source": [
    "for (c, _) in cnts:\n",
    "    # un pack the contuors\n",
    "    (x, y, w, h) = cv2.boundingRect(c)\n",
    "\n",
    "    if w >= 7 and h >= 20:\n",
    "        # Crop the digit in the image\n",
    "        roi = gray[y:y + h, x:x + w]\n",
    "        # Copy the image\n",
    "        thresh = roi.copy()\n",
    "        # find the threshold value using Otsu\n",
    "        T = mahotas.thresholding.otsu(roi)\n",
    "        # Replace 255 if the pixels exceds the threshold vlue\n",
    "        thresh[thresh > T] = 255\n",
    "        # Change black to white and white to black\n",
    "        thresh = cv2.bitwise_not(thresh)\n",
    "        \n",
    "        # Deskew the digit in the image\n",
    "        thresh = deskew(thresh, 50)\n",
    "        # place it in the center\n",
    "        thresh = center_extent(thresh, (50, 50))\n",
    "        \n",
    "        # Show the digit \n",
    "        #cv2.imshow(\"thresh\", thresh)\n",
    "        # find features for the digit\n",
    "        hist = hog.describe(thresh)\n",
    "        # Predict the digit using the trained model\n",
    "        digit = model.predict([hist])[0]\n",
    "        print(\"I think that number is: {}\" .format(digit))\n",
    "        \n",
    "        # Make the rectangle on the digit\n",
    "        cv2.rectangle(image, (x, y), (x+w, y+h), (0, 255, 0), 1)\n",
    "        # Show the predicted image's digit on the left top \n",
    "        cv2.putText(image, str(digit), (x-10, y-10), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 0), 2)\n",
    "        # Show the image\n",
    "        cv2.imshow(\"image\", image)\n",
    "        cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d97e523",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fac8f10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
